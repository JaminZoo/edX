library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("SnowballC", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
frequencies = DocumentTermMatrix
tweets = read.csv("tweets.csv", stringsAsFactors = F) # always need the 2nd argument when dealing with text based data
setwd("~/Documents/edX/Unit 5 - Text Analytics")
tweets = read.csv("tweets.csv", stringsAsFactors = F) # always need the 2nd argument when dealing with text based data
tweets$Negative = as.factor(tweets$Avg <= -1)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus[[1]] # outputs the first tweet of the corpus, with all words converted to lower case
corpus = tm_map(corpus, removePunctuation) # removes all punctuation from corpus
Sys.setlocale("LC_ALL", "C") # Changes location to USA
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english"))) # this does not remove the word 'apple' from combined words e.g. thanxapple
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix
frequencies
frequencies = DocumentTermMatrix(corpus)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation) # removes all punctuation from corpus
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english"))) # this does not remove the word 'apple' from combined words e.g. thanxapple
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
frequencies
inspect(frequencies[1000:1005,505:515])
corpus[[1]] # outputs the first tweet of the corpus, with all words converted to lower case
install.packages("NPL")
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removePunctuation) # removes all punctuation from corpus
Sys.setlocale("LC_ALL", "C") # Changes location to USA
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english"))) # this does not remove the word 'apple' from combined words e.g. thanxapple
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
inspect(frequencies[1000:1005,505:515])
corpus[[1]]
findFreqTerms(corpus, lowfreq = 20)
findFreqTerms(frequencies, lowfreq = 20)
?sparse
?sparseterms
?removesparseterms
sparse = removeSparseTerms(frequencies, 0.995)
sparse
?as.matrix
tweetSparse = as.factor(as.matrix(sparse))
str(tweetSparse)
tweetsSparse = as.factor(as.matrix(sparse))
rm(tweetSparse)
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
?as.data.frame
tweetsSparse = as.data.factor(as.matrix(sparse))
tweetsSparse = as.data.frame(as.matrix(sparse))
View(tweetsSparse)
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
View(tweetsSparse)
tweetsSparse$Negative = tweets$Negative
View(tweetsSparse)
str(tweetsSparse)
summary(tweetsSparse)
summary(tweets)
?sample.split
?split
library("caTools", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
Train = subset(tweetsSparse, split = T)
Test = subset(tweetsSparse, split = F)
Train = subset(tweetsSparse, split == T)
Test = subset(tweetsSparse, split == F)
Train[c(1:2)]
Train[1,c(1:2)]
Train[1,c(1:5)]
Train[1,c(1:6)]
Train[500,c(1:6)]
which.max(tweetsSparse)
which.max(tweetsSparse$Negative)
tweetsSparse[1000]
tweetsSparse[1000,]
findFreqTerms(frequencies, lowfreq = 100)
findFreqTerms(frequencies, lowfreq = 150)
trainSparse = subset(tweetsSparse, split == T)
testSparse = subset(tweetsSparse, split == F)
rm(Train, Test)
tweetCART = rpart(Negative ~ ., data = trainSparse, method = "class")
library("rpart", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ROCR", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
tweetCART = rpart(Negative ~ ., data = trainSparse, method = "class")
summary(tweetCART)
prp(tweetCART)
library("rpart.plot", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
prp(tweetCART)
tweetPreidc = predict(tweetCART, newdata = testSparse, type = 'class')
tweetPredic = predict(tweetCART, newdata = testSparse, type = 'class')
table(testSparse$Negative, tweetPredic)
37+18+6+294
312/355
table(tweetsSparse$Negative)
table(testSparse$Negative)
300/355
library("randomForest", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
set.seed(123)
tweetRF = randomForest(Negative ~ ., data = trainSparse)
predicRF = predict(tweetRF, newdata = testSparse)
table(testSparse$Negative, predicRF)
34+21+7+293
21
314/355
tweetLogit = glm(Negative ~ ., data = trainSparse, family = "binomial")
predicLogit = predict(tweetsSparse$Negative, tweetLogit)
predicLogit = predict(tweetLogit, newdata = testSparse, type = response)
predicLogit = predict(tweetLogit, newdata = testSparse, type = "response")
table(tweetsSparse$Negative, predicLogit)
table(tweetsSparse$Negative, predicLogit > 0.5)
table(testSparse$Negative, predicLogit > 0.5)
32+253
285/355
set.seed(123)
tweetRF = randomForest(Negative ~ ., data = trainSparse)
predicRF = predict(tweetRF, newdata = testSparse)
table(testSparse$Negative, predicRF)
tweetLogit = glm(Negative ~ ., data = trainSparse, family = "binomial")
predicLogit = predict(tweetLogit, newdata = testSparse, type = "response")
table(testSparse$Negative, predicLogit > 0.5)
emails = read.csv("energy_bids.csv", stringsAsFactors = F)
str(emails)
emails$email
emails$email[1]
View(emails)
View(emails)
str(emails)
summary(emails$responsive)
strwrap(emails$email[1])
strwrap(emails$email[2])
corpus = Corpus(VectorSource(emails$email))
corpus[[1]]
strwrap(corpus[1])
strwrap(corpus[[1]]
strwrap(corpus[[1]])
strwrap(corpus[[1]])
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
strwrap(corpus[[1]])
dtm = DocumentTermMatrix(corpus)
dtm
sparse = removeSparseTerms(dtm, 0.97)
sparse
emailsSparse = as.data.frame(as.matrix(sparse))
View(emailsSparse)
emailsSparse$responsive = emails$responsive
str(emailsSparse)
View(emailsSparse)
tail(str(emailsSparse))
rm(tweetLogit, tweetRF, tweeCART, predicRF, predicLogit)
rm(tweetCART)
rm(tweetPreidc, tweetPreidc)
rm(tweets, tweetsSparse, testSparse, trainSparse)
str(emailsSparse)
set.seed(144)
split = sample.split(emailsSparse$responsive, SplitRatio = 0.7)
trainSparse = subset(emailsSparse, split == T)
testSparse = subset(emailsSparse, split == F)
emailsCART = rpart(responsive ~ ., data = trainSparse, method = "class")
prp(emailsCART)
cartPredic = predict(emailsCART, newdata = testSparse, type = 'classl')
cartPredic = predict(emailsCART, newdata = testSparse, type = 'class')
table(testSparse$responsive, cartPredic)
?prob
predicProb = cartPredic[,2]
cartPredic.prob = cartPredic[,2]
table(testSparse$responsive, cartPredic[,2] >= 0.5)
table(testSparse$responsive, cartPredic)
17+25+20+195
220/257
table(testSparse$responsive)
predicROCR = prediction(cartPredic, testSparse$responsive)
perfROCR = performance(predicRORC, 'tpr', 'fpr')
perfROCR = performance(predicROCR, 'tpr', 'fpr')
predicROCR = prediction(cartPredic.prob, testSparse$responsive)
cartPredic.prob = cartPredic[,2]
predic = predict(emailsCART, newdata = testSparse)
predic[1:10,] # display first 10 rows of the prediction, first col shows words that are not responsive and the 2nd row those that are responsive
predic.prob = cartPredic[,2]
predic.prob = predc[,2]
predic.prob = predic[,2]
table(testSparse$responsive, predic >= 0.5)
table(testSparse$responsive, predic.prob >= 0.5)
predicROCR = prediction(predic.prob, testSparse$responsive)
perfROCR = performance(predicROCR, 'tpr', 'fpr')
auc = as.numeric(performance(predicROCR, "auc")@y.values)
auc
plot(perfROCR, colorize = T)
?stringAsFactors
?read.csv
wiki = read.csv("wiki.csv", stringsAsFactors = F)
